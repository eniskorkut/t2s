"""
Vanna AI Configuration Module
Contains the MyVanna class that combines ChromaDB and Ollama for RAG-based Text-to-SQL.
"""
from vanna.legacy.ollama import Ollama
from vanna.legacy.chromadb import ChromaDB_VectorStore
from chromadb.utils import embedding_functions


class MyVanna(ChromaDB_VectorStore, Ollama):
    """Custom Vanna class combining ChromaDB for vector storage and Ollama for LLM."""
    
    def __init__(self, config=None):
        if config is None:
            config = {}
        
        # Set default config values
        config.setdefault("ollama_host", "http://ollama:11434")
        config.setdefault("model", "qwen2.5-coder:3b")
        config.setdefault("path", "./chroma_db")
        # Increase timeout for model pulling (10 minutes = 600 seconds)
        config.setdefault("ollama_timeout", 600.0)
        
        # Initialize Multilingual Embedding Function
        # This replaces the default English-only embedding model
        # paraphrase-multilingual-MiniLM-L12-v2 works great for Turkish and is faster
        print("ðŸ”§ Initializing Multilingual Embedding Model...")
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="paraphrase-multilingual-MiniLM-L12-v2"
        )
        config["embedding_function"] = embedding_function
        
        # Initialize parent classes
        ChromaDB_VectorStore.__init__(self, config=config)
        
        # Initialize Ollama with error handling for model pulling
        try:
            Ollama.__init__(self, config=config)
        except Exception as e:
            # If model pulling fails (timeout, network issues, etc.), 
            # try to continue anyway - model might already be available
            error_msg = str(e)
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                print(f"âš ï¸  Warning: Model pull timed out during initialization.")
                print(f"   Model: {config.get('model')}")
                print(f"   The model will be pulled automatically on first use if needed.")
                print(f"   Continuing with initialization...")
                
                # Manually initialize Ollama without pulling model
                self._init_ollama_without_pull(config)
            else:
                # For other errors, re-raise
                raise

    def get_similar_question_sql(self, question: str, **kwargs) -> list:
        """Limit similar questions to 3 for context reduction (performance optimization)."""
        return super().get_similar_question_sql(question, n_results=3, **kwargs)

    def get_related_ddl(self, question: str, **kwargs) -> list:
        """Limit related DDL to 3 for context reduction (performance optimization)."""
        return super().get_related_ddl(question, n_results=3, **kwargs)

    def get_related_documentation(self, question: str, **kwargs) -> list:
        """Limit related documentation to 2 for context reduction (performance optimization)."""
        return super().get_related_documentation(question, n_results=2, **kwargs)
    
    def generate_sql_stream(self, question: str, **kwargs):
        """
        Generate SQL with streaming support.
        Yields tokens as they are generated by Ollama.
        """
        # Step 1: Get context (training data)
        similar_sql = self.get_similar_question_sql(question, **kwargs)
        related_ddl = self.get_related_ddl(question, **kwargs)
        related_doc = self.get_related_documentation(question, **kwargs)

        # Step 2: Build prompt
        prompt = self.get_sql_prompt(
            question=question,
            question_sql_list=similar_sql,
            ddl_list=related_ddl,
            doc_list=related_doc,
            initial_prompt=None,
            **kwargs
        )

        # Step 3: Stream from Ollama
        try:
            # Check if ollama_client is available (it should be if __init__ finished)
            if not hasattr(self, 'ollama_client'):
                # Initialize if needed (e.g. if it was initialized without pull)
                config = {
                    "ollama_host": self.host if hasattr(self, 'host') else "http://ollama:11434",
                    "model": self.model if hasattr(self, 'model') else "qwen2.5-coder:3b",
                    "ollama_timeout": self.ollama_timeout if hasattr(self, 'ollama_timeout') else 600.0
                }
                self._init_ollama_without_pull(config)

            response_stream = self.ollama_client.chat(
                model=self.model,
                messages=prompt,
                stream=True,
                options=self.ollama_options if hasattr(self, 'ollama_options') else {}
            )

            for chunk in response_stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    yield chunk['message']['content']
        except Exception as e:
            print(f"Error in streaming SQL generation: {e}")
            yield f"Error: {str(e)}"

    def _init_ollama_without_pull(self, config):
        """Initialize Ollama client without pulling model."""
        import ollama
        from httpx import Timeout
        
        self.host = config.get("ollama_host", "http://ollama:11434")
        self.model = config["model"]
        if ":" not in self.model:
            self.model += ":latest"
        
        self.ollama_timeout = config.get("ollama_timeout", 600.0)
        self.ollama_client = ollama.Client(
            self.host, timeout=Timeout(self.ollama_timeout)
        )
        self.keep_alive = config.get("keep_alive", None)
        self.ollama_options = config.get("options", {})
        self.num_ctx = self.ollama_options.get("num_ctx", 2048)
        
        # Set log function (ChromaDB_VectorStore already has log from parent class)
        # If not available, use a simple print-based logger
        if not hasattr(self, 'log'):
            self.log = lambda x: print(f"[Vanna] {x}") if x else None


def get_default_config():
    """Returns default configuration for Vanna AI."""
    return {
        "ollama_host": "http://ollama:11434",
        "model": "qwen2.5-coder:3b",
        "path": "./chroma_db"
    }
